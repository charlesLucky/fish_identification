{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import  tensorflow as tf\n",
    "from    tensorflow import  keras\n",
    "from    tensorflow.keras import datasets, layers, optimizers, models\n",
    "from    tensorflow.keras import regularizers\n",
    "import  os\n",
    "\n",
    "import  argparse\n",
    "import  numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import IPython.display as display\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "#from tensorflow.keras.preprocessing import image\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(models.Model):\n",
    "\n",
    "\n",
    "    def __init__(self, input_shape,num_classes):\n",
    "        \"\"\"\n",
    "\n",
    "        :param input_shape: [32, 32, 3]\n",
    "        \"\"\"\n",
    "        super(VGG16, self).__init__()\n",
    "\n",
    "        weight_decay = 0.000\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        model = models.Sequential()\n",
    "\n",
    "        model.add(layers.Conv2D(64, (3, 3), padding='same',\n",
    "                         input_shape=input_shape, kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(layers.Activation('relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(0.3))\n",
    "\n",
    "        model.add(layers.Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(layers.Activation('relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        model.add(layers.Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(layers.Activation('relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(0.4))\n",
    "\n",
    "        model.add(layers.Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(layers.Activation('relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        model.add(layers.Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(layers.Activation('relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(0.4))\n",
    "\n",
    "        model.add(layers.Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(layers.Activation('relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(0.4))\n",
    "\n",
    "        model.add(layers.Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(layers.Activation('relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "        model.add(layers.Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(layers.Activation('relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(0.4))\n",
    "\n",
    "        model.add(layers.Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(layers.Activation('relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(0.4))\n",
    "\n",
    "        model.add(layers.Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(layers.Activation('relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "        model.add(layers.Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(layers.Activation('relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(0.4))\n",
    "\n",
    "        model.add(layers.Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(layers.Activation('relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(0.4))\n",
    "\n",
    "        model.add(layers.Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(layers.Activation('relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(layers.Dropout(0.5))\n",
    "\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(512,kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(layers.Activation('relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "\n",
    "        model.add(layers.Dropout(0.5))\n",
    "        model.add(layers.Dense(self.num_classes))\n",
    "        # model.add(layers.Activation('softmax'))\n",
    "\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "\n",
    "        x = self.model(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X_train, X_test):\n",
    "    # this function normalize inputs for zero mean and unit variance\n",
    "    # it is used when training a model.\n",
    "    # Input: training set and test set\n",
    "    # Output: normalized training set and test set according to the trianing set statistics.\n",
    "    X_train = X_train / 255.\n",
    "    X_test = X_test / 255.\n",
    "\n",
    "    mean = np.mean(X_train, axis=(0, 1, 2, 3))\n",
    "    std = np.std(X_train, axis=(0, 1, 2, 3))\n",
    "    print('mean:', mean, 'std:', std)\n",
    "    X_train = (X_train - mean) / (std + 1e-7)\n",
    "    X_test = (X_test - mean) / (std + 1e-7)\n",
    "    return X_train, X_test\n",
    "\n",
    "def prepare_cifar(x, y):\n",
    "\n",
    "    x = tf.cast(x, tf.float32)\n",
    "    y = tf.cast(y, tf.int32)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "\n",
    "def compute_loss(logits, labels):\n",
    "  return tf.reduce_mean(\n",
    "      tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "          logits=logits, labels=labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "#data_dir = tf.keras.utils.get_file(origin='SESSION1_ROI',fname='SESSION1_ROI')\n",
    "#data_dir = pathlib.Path(data_dir)\n",
    "#data_dir = pathlib.Path('/home/xingbo/Desktop/fish_identification/data/SESSION_TENT/SESSION1')\n",
    "data_dir = pathlib.Path('/home/xingbo/Desktop/fish_identification/data/SESSION_AQUARIUM/SESSION1')\n",
    "data_dir\n",
    "image_count = len(list(data_dir.glob('*/*.png')))\n",
    "image_count\n",
    "CLASS_NAMES = np.array([item.name for item in data_dir.glob('*') if item.name != \"LICENSE.txt\"])\n",
    "class_num=len(CLASS_NAMES)\n",
    "\n",
    "CLASS_NAMES[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "IMG_SIZE=160\n",
    "IMG_HEIGHT = IMG_SIZE\n",
    "IMG_WIDTH = IMG_SIZE\n",
    "STEPS_PER_EPOCH = np.ceil(image_count/BATCH_SIZE)\n",
    "\n",
    "list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "  # convert the path to a list of path components\n",
    "  parts = tf.strings.split(file_path, '/')\n",
    "  # The second to last is the class-directory\n",
    "  print(parts[-2] == CLASS_NAMES)\n",
    "  wh = tf.where(tf.equal(CLASS_NAMES,parts[-2]))\n",
    "  return wh\n",
    "def decode_img(img):\n",
    "  # convert the compressed string to a 3D uint8 tensor\n",
    "  img = tf.image.decode_jpeg(img, channels=3)\n",
    "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "  #img = (img/127.5) - 1\n",
    "  # resize the image to the desired size.\n",
    "  return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
    "\n",
    "def process_path(file_path):\n",
    "  label = get_label(file_path)\n",
    "  # load the raw data from the file as a string\n",
    "  img = tf.io.read_file(file_path)\n",
    "  #img = Image.open(file_path)\n",
    "  #img = image.load_img(file_path)\n",
    "  #img = img.convert('RGB')\n",
    "  img = decode_img(img)\n",
    "  return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "labeled_ds = list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "image_batch, label_batch = next(iter(labeled_ds))\n",
    "label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n",
    "  # This is a small dataset, only load it once, and keep it in memory.\n",
    "  # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "  # fit in memory.\n",
    "  if cache:\n",
    "    if isinstance(cache, str):\n",
    "      ds = ds.cache(cache)\n",
    "    else:\n",
    "      ds = ds.cache()\n",
    "\n",
    "  ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "  # Repeat forever\n",
    "  ds = ds.repeat()\n",
    "\n",
    "  ds = ds.batch(BATCH_SIZE)\n",
    "\n",
    "  # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "  # is training.\n",
    "  ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * image_count)\n",
    "val_size = int(0.15 * image_count)\n",
    "test_size = int(0.15 * image_count)\n",
    "train_ds = prepare_for_training(labeled_ds)\n",
    "\n",
    "full_dataset = train_ds.shuffle(buffer_size=1000,reshuffle_each_iteration = False )\n",
    "train_dataset = full_dataset.take(train_size)\n",
    "test_dataset = full_dataset.skip(train_size)\n",
    "val_dataset = test_dataset.skip(val_size)\n",
    "test_dataset = test_dataset.take(test_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    tf.random.set_seed(22)\n",
    "    image_batch, label_batch = next(iter(train_dataset))\n",
    "    print('loading data...')\n",
    "    (x,y) = image_batch,label_batch\n",
    "    image_batch, label_batch = next(iter(val_dataset))\n",
    "    (x_test, y_test)= image_batch,label_batch\n",
    "    print(x.shape, y.shape, x_test.shape, y_test.shape)\n",
    "    # x = tf.convert_to_tensor(x)\n",
    "    # y = tf.convert_to_tensor(y)\n",
    "    train_loader = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "    train_loader = train_loader.map(prepare_cifar).shuffle(50000).batch(256)\n",
    "\n",
    "    test_loader = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    test_loader = test_loader.map(prepare_cifar).shuffle(10000).batch(256)\n",
    "    print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "    model = VGG16([IMG_SIZE, IMG_SIZE, 3],class_num)\n",
    "\n",
    "\n",
    "    # must specify from_logits=True!\n",
    "    criteon = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    metric = keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "    optimizer = optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "\n",
    "    for epoch in range(250):\n",
    "\n",
    "        for step, (x, y) in enumerate(train_loader):\n",
    "            # [b, 1] => [b]\n",
    "            y = tf.squeeze(y, axis=1)\n",
    "            # [b, 10]\n",
    "            y = tf.one_hot(y, depth=class_num)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(x)\n",
    "                loss = criteon(y, logits)\n",
    "                # loss2 = compute_loss(logits, tf.argmax(y, axis=1))\n",
    "                # mse_loss = tf.reduce_sum(tf.square(y-logits))\n",
    "                # print(y.shape, logits.shape)\n",
    "                metric.update_state(y, logits)\n",
    "\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            # MUST clip gradient here or it will disconverge!\n",
    "            grads = [ tf.clip_by_norm(g, 15) for g in grads]\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "            if step % 40 == 0:\n",
    "                # for g in grads:\n",
    "                #     print(tf.norm(g).numpy())\n",
    "                print(epoch, step, 'loss:', float(loss), 'acc:', metric.result().numpy())\n",
    "                metric.reset_states()\n",
    "\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "\n",
    "            metric = keras.metrics.CategoricalAccuracy()\n",
    "            for x, y in test_loader:\n",
    "                # [b, 1] => [b]\n",
    "                y = tf.squeeze(y, axis=1)\n",
    "                # [b, 10]\n",
    "                y = tf.one_hot(y, depth=class_num)\n",
    "\n",
    "                logits = model.predict(x)\n",
    "                # be careful, these functions can accept y as [b] without warnning.\n",
    "                metric.update_state(y, logits)\n",
    "            print('test acc:', metric.result().numpy())\n",
    "            metric.reset_states()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
