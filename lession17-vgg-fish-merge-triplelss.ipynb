{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  os\n",
    "import  tensorflow as tf\n",
    "import  numpy as np\n",
    "from    tensorflow import keras\n",
    "from utils import LoadFishDataUtil\n",
    "from    tensorflow.keras import datasets, layers, optimizers\n",
    "from    tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import pathlib\n",
    "import h5py\n",
    "## dataset\n",
    "\n",
    "## for Model definition/training\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, concatenate,  Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "## required for semi-hard triplet loss:\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from    tensorflow.keras import datasets, layers, optimizers, models\n",
    "from    tensorflow.keras import regularizers\n",
    "\n",
    "## for visualizing \n",
    "import matplotlib.pyplot as plt, numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from itertools import islice\n",
    "\n",
    "le = LabelEncoder()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### include the necessary functions for triplet loss:\n",
    "** (from TF libraries) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distance(feature, squared=False):\n",
    "    \"\"\"Computes the pairwise distance matrix with numerical stability.\n",
    "\n",
    "    output[i, j] = || feature[i, :] - feature[j, :] ||_2\n",
    "\n",
    "    Args:\n",
    "      feature: 2-D Tensor of size [number of data, feature dimension].\n",
    "      squared: Boolean, whether or not to square the pairwise distances.\n",
    "\n",
    "    Returns:\n",
    "      pairwise_distances: 2-D Tensor of size [number of data, number of data].\n",
    "    \"\"\"\n",
    "    pairwise_distances_squared = math_ops.add(\n",
    "        math_ops.reduce_sum(math_ops.square(feature), axis=[1], keepdims=True),\n",
    "        math_ops.reduce_sum(\n",
    "            math_ops.square(array_ops.transpose(feature)),\n",
    "            axis=[0],\n",
    "            keepdims=True)) - 2.0 * math_ops.matmul(feature,\n",
    "                                                    array_ops.transpose(feature))\n",
    "\n",
    "    # Deal with numerical inaccuracies. Set small negatives to zero.\n",
    "    pairwise_distances_squared = math_ops.maximum(pairwise_distances_squared, 0.0)\n",
    "    # Get the mask where the zero distances are at.\n",
    "    error_mask = math_ops.less_equal(pairwise_distances_squared, 0.0)\n",
    "\n",
    "    # Optionally take the sqrt.\n",
    "    if squared:\n",
    "        pairwise_distances = pairwise_distances_squared\n",
    "    else:\n",
    "        pairwise_distances = math_ops.sqrt(\n",
    "            pairwise_distances_squared + math_ops.to_float(error_mask) * 1e-16)\n",
    "\n",
    "    # Undo conditionally adding 1e-16.\n",
    "    pairwise_distances = math_ops.multiply(\n",
    "        pairwise_distances, math_ops.to_float(math_ops.logical_not(error_mask)))\n",
    "\n",
    "    num_data = array_ops.shape(feature)[0]\n",
    "    # Explicitly set diagonals to zero.\n",
    "    mask_offdiagonals = array_ops.ones_like(pairwise_distances) - array_ops.diag(\n",
    "        array_ops.ones([num_data]))\n",
    "    pairwise_distances = math_ops.multiply(pairwise_distances, mask_offdiagonals)\n",
    "    return pairwise_distances\n",
    "\n",
    "def masked_maximum(data, mask, dim=1):\n",
    "    \"\"\"Computes the axis wise maximum over chosen elements.\n",
    "\n",
    "    Args:\n",
    "      data: 2-D float `Tensor` of size [n, m].\n",
    "      mask: 2-D Boolean `Tensor` of size [n, m].\n",
    "      dim: The dimension over which to compute the maximum.\n",
    "\n",
    "    Returns:\n",
    "      masked_maximums: N-D `Tensor`.\n",
    "        The maximized dimension is of size 1 after the operation.\n",
    "    \"\"\"\n",
    "    axis_minimums = math_ops.reduce_min(data, dim, keepdims=True)\n",
    "    masked_maximums = math_ops.reduce_max(\n",
    "        math_ops.multiply(data - axis_minimums, mask), dim,\n",
    "        keepdims=True) + axis_minimums\n",
    "    return masked_maximums\n",
    "\n",
    "def masked_minimum(data, mask, dim=1):\n",
    "    \"\"\"Computes the axis wise minimum over chosen elements.\n",
    "\n",
    "    Args:\n",
    "      data: 2-D float `Tensor` of size [n, m].\n",
    "      mask: 2-D Boolean `Tensor` of size [n, m].\n",
    "      dim: The dimension over which to compute the minimum.\n",
    "\n",
    "    Returns:\n",
    "      masked_minimums: N-D `Tensor`.\n",
    "        The minimized dimension is of size 1 after the operation.\n",
    "    \"\"\"\n",
    "    axis_maximums = math_ops.reduce_max(data, dim, keepdims=True)\n",
    "    masked_minimums = math_ops.reduce_min(\n",
    "        math_ops.multiply(data - axis_maximums, mask), dim,\n",
    "        keepdims=True) + axis_maximums\n",
    "    return masked_minimums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our Triplet Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss_adapted_from_tf(y_true, y_pred):\n",
    "    del y_true\n",
    "    margin = 1.\n",
    "    labels = y_pred[:, :1]\n",
    "\n",
    " \n",
    "    labels = tf.cast(labels, dtype='int32')\n",
    "\n",
    "    embeddings = y_pred[:, 1:]\n",
    "\n",
    "    ### Code from Tensorflow function [tf.contrib.losses.metric_learning.triplet_semihard_loss] starts here:\n",
    "    \n",
    "    # Reshape [batch_size] label tensor to a [batch_size, 1] label tensor.\n",
    "    # lshape=array_ops.shape(labels)\n",
    "    # assert lshape.shape == 1\n",
    "    # labels = array_ops.reshape(labels, [lshape[0], 1])\n",
    "\n",
    "    # Build pairwise squared distance matrix.\n",
    "    pdist_matrix = pairwise_distance(embeddings, squared=True)\n",
    "    # Build pairwise binary adjacency matrix.\n",
    "    adjacency = math_ops.equal(labels, array_ops.transpose(labels))\n",
    "    # Invert so we can select negatives only.\n",
    "    adjacency_not = math_ops.logical_not(adjacency)\n",
    "\n",
    "    # global batch_size  \n",
    "    batch_size = array_ops.size(labels) # was 'array_ops.size(labels)'\n",
    "\n",
    "    # Compute the mask.\n",
    "    pdist_matrix_tile = array_ops.tile(pdist_matrix, [batch_size, 1])\n",
    "    mask = math_ops.logical_and(\n",
    "        array_ops.tile(adjacency_not, [batch_size, 1]),\n",
    "        math_ops.greater(\n",
    "            pdist_matrix_tile, array_ops.reshape(\n",
    "                array_ops.transpose(pdist_matrix), [-1, 1])))\n",
    "    mask_final = array_ops.reshape(\n",
    "        math_ops.greater(\n",
    "            math_ops.reduce_sum(\n",
    "                math_ops.cast(mask, dtype=dtypes.float32), 1, keepdims=True),\n",
    "            0.0), [batch_size, batch_size])\n",
    "    mask_final = array_ops.transpose(mask_final)\n",
    "\n",
    "    adjacency_not = math_ops.cast(adjacency_not, dtype=dtypes.float32)\n",
    "    mask = math_ops.cast(mask, dtype=dtypes.float32)\n",
    "\n",
    "    # negatives_outside: smallest D_an where D_an > D_ap.\n",
    "    negatives_outside = array_ops.reshape(\n",
    "        masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size])\n",
    "    negatives_outside = array_ops.transpose(negatives_outside)\n",
    "\n",
    "    # negatives_inside: largest D_an.\n",
    "    negatives_inside = array_ops.tile(\n",
    "        masked_maximum(pdist_matrix, adjacency_not), [1, batch_size])\n",
    "    semi_hard_negatives = array_ops.where(\n",
    "        mask_final, negatives_outside, negatives_inside)\n",
    "\n",
    "    loss_mat = math_ops.add(margin, pdist_matrix - semi_hard_negatives)\n",
    "\n",
    "    mask_positives = math_ops.cast(\n",
    "        adjacency, dtype=dtypes.float32) - array_ops.diag(\n",
    "        array_ops.ones([batch_size]))\n",
    "\n",
    "    # In lifted-struct, the authors multiply 0.5 for upper triangular\n",
    "    #   in semihard, they take all positive pairs except the diagonal.\n",
    "    num_positives = math_ops.reduce_sum(mask_positives)\n",
    "\n",
    "    semi_hard_triplet_loss_distance = math_ops.truediv(\n",
    "        math_ops.reduce_sum(\n",
    "            math_ops.maximum(\n",
    "                math_ops.multiply(loss_mat, mask_positives), 0.0)),\n",
    "        num_positives,\n",
    "        name='triplet_semihard_loss')\n",
    "    \n",
    "    ### Code from Tensorflow function semi-hard triplet loss ENDS here.\n",
    "    return semi_hard_triplet_loss_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_network(input_shape, embedding_size):\n",
    "    \"\"\"\n",
    "    Base network to be shared (eq. to feature extraction).\n",
    "    \"\"\"\n",
    "\n",
    "    weight_decay = 0.000\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3, 3), padding='same', input_shape=input_shape,\n",
    "                            kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(256, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    model.add(layers.Conv2D(256, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    model.add(layers.Conv2D(256, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(512, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    model.add(layers.Conv2D(512, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    model.add(layers.Conv2D(512, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(512, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    model.add(layers.Conv2D(512, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    model.add(layers.Conv2D(512, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    #model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1024, kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(embedding_size))\n",
    "    # model.add(layers.Activation('softmax'))\n",
    "    plot_model(model, to_file='triple_base_network.png', show_shapes=True, show_layer_names=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the training/validation/testing DATA, as well as some other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total images: 3327\n"
     ]
    }
   ],
   "source": [
    "#data_dir ='/media/xingbo/Storage/fish_identification/data/SESSION_TENT/SESSION1'\n",
    "data_dir ='/media/xingbo/Storage/fish_identification/data/SESSION_AQUARIUM/SESSION_MERGE/SESSION1'\n",
    "data_dir_path = pathlib.Path(data_dir)\n",
    "image_count = len(list(data_dir_path.glob('*/*.png')))\n",
    "print('total images:',image_count)\n",
    "BATCH_SIZE = 8\n",
    "IMG_SIZE=160\n",
    "IMG_WIDTH=320\n",
    "IMG_HEIGHT=60\n",
    "IMG_TYPES = ['.png', '.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total class: 328\n"
     ]
    }
   ],
   "source": [
    "CLASS_NAMES=None\n",
    "SPLIT_WEIGHTS=(0.7, 0.3, 0.0)# train cv val vs test\n",
    "myloadData = LoadFishDataUtil(data_dir,BATCH_SIZE,IMG_WIDTH,IMG_HEIGHT,CLASS_NAMES,SPLIT_WEIGHTS)\n",
    "train_dataset,val_dataset,test_dataset,STEPS_PER_EPOCH, CLASS_NAMES,class_num = myloadData.loadFishDataWithname()\n",
    "input_shape=(IMG_WIDTH,IMG_HEIGHT, 3)\n",
    "print('total class:',class_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_num_elements 2328, val_dataset_num_elements 998, test_dataset_num_elements 0\n"
     ]
    }
   ],
   "source": [
    "train_dataset_num_elements = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "val_dataset_num_elements = tf.data.experimental.cardinality(val_dataset).numpy()\n",
    "test_dataset_num_elements = tf.data.experimental.cardinality(test_dataset).numpy()\n",
    "print(f\"train_dataset_num_elements {train_dataset_num_elements}, val_dataset_num_elements {val_dataset_num_elements}, test_dataset_num_elements {test_dataset_num_elements}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18624\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-506487325106>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "x_train = []\n",
    "y_train = []\n",
    "inc = iter(train_dataset)\n",
    "while True:\n",
    "    try:\n",
    "        image_batch, label_batch = next(inc)\n",
    "        imgs = image_batch.numpy()\n",
    "        labels = label_batch.numpy()\n",
    "        for i in range(len(imgs)):\n",
    "            x_train.append(imgs[i,:,:,:])\n",
    "            y_train.append(labels[i])\n",
    "            n = n + 1\n",
    "            #print(n)\n",
    "    except:\n",
    "        break\n",
    "   \n",
    "\n",
    "print(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "\n",
    "inc = iter(val_dataset)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        image_batch, label_batch = next(inc)\n",
    "        imgs = image_batch.numpy()\n",
    "        labels = label_batch.numpy()\n",
    "        for i in range(len(imgs)):\n",
    "            if n ==0:\n",
    "                x_val = np.array(imgs[i])\n",
    "                y_val = np.array(labels[i])\n",
    "                x_val = np.expand_dims(x_val,-1)\n",
    "                y_val = np.expand_dims(y_val,-1)\n",
    "            else:\n",
    "                x_val = np.append(x_val, imgs[i], axis = 0)\n",
    "                y_val = np.append(y_val, labels[i], axis = 0)\n",
    "            n = n + 1\n",
    "            print(n)\n",
    "    except:\n",
    "        break\n",
    "   \n",
    "\n",
    "print(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 320, 60, 3)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(x_train.numpy())\n",
    "print()\n",
    "print(y_train.numpy())\n",
    "new_x=[x_train, y_train ]\n",
    "print(new_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_image (InputLayer)        [(None, 320, 60, 3)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_label (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 128)          46325184    input_image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 129)          0           input_label[0][0]                \n",
      "                                                                 sequential_1[1][0]               \n",
      "==================================================================================================\n",
      "Total params: 46,325,184\n",
      "Trainable params: 46,314,688\n",
      "Non-trainable params: 10,496\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "epochs = 25\n",
    "train_flag = True  # either     True or False\n",
    "embedding_size = 128\n",
    "    \n",
    "base_network = create_base_network(input_shape, embedding_size)\n",
    "\n",
    "input_images = Input(shape=input_shape, name='input_image')  # input layer for images\n",
    "input_labels = Input(shape=(1,), name='input_label')  # input layer for labels\n",
    "embeddings = base_network([input_images])  # output of network -> embeddings\n",
    "labels_plus_embeddings = concatenate([input_labels, embeddings])  # concatenating the labels + embeddings\n",
    "\n",
    "# Defining a model with inputs (images, labels) and outputs (labels_plus_embeddings)\n",
    "model = Model(inputs=[input_images, input_labels],\n",
    "              outputs=labels_plus_embeddings)\n",
    "\n",
    "model.summary()\n",
    "#plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 460800 into shape (8,320,320,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'reshape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-2dceb9b8d154>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdummy_gt_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mx_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    290\u001b[0m            [5, 6]])\n\u001b[1;32m    291\u001b[0m     \"\"\"\n\u001b[0;32m--> 292\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# a downstream library like 'pandas'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 460800 into shape (8,320,320,1)"
     ]
    }
   ],
   "source": [
    "# train session\n",
    "opt = Adam(lr=0.0001)  # choose optimiser. RMS is good too!\n",
    "\n",
    "model.compile(loss=triplet_loss_adapted_from_tf,\n",
    "              optimizer=opt)\n",
    "\n",
    "filepath = \"model/semiH_trip_fish_ep{epoch:02d}_BS%d.hdf5\" % batch_size\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, period=25)\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "        # Uses 'dummy' embeddings + dummy gt labels. Will be removed as soon as loaded, to free memory\n",
    "dummy_gt_train = np.zeros((len(x_train), embedding_size + 1))\n",
    "dummy_gt_val = np.zeros((len(x_val), embedding_size + 1))\n",
    "\n",
    "x_train = np.reshape(x_train, (len(y_train), x_train.shape[1], x_train.shape[1], 1))\n",
    "x_val = np.reshape(x_val, (len(y_val), x_train.shape[1], x_train.shape[1], 1))\n",
    "\n",
    "\n",
    "x_train, y_train = next(iter(train_dataset))\n",
    "x_val, y_val = next(iter(val_dataset))\n",
    "\n",
    "# Uses 'dummy' embeddings + dummy gt labels. Will be removed as soon as loaded, to free memory\n",
    "dummy_gt_train = np.zeros((train_dataset_num_elements, embedding_size + 1))\n",
    "dummy_gt_val = np.zeros((val_dataset_num_elements, embedding_size + 1))\n",
    "\n",
    "H = model.fit(\n",
    "    x=[x_train,y_train],\n",
    "    y=dummy_gt_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=([x_val, y_val], dummy_gt_val),\n",
    "    callbacks=callbacks_list)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(H.history['loss'], label='training loss')\n",
    "plt.plot(H.history['val_loss'], label='validation loss')\n",
    "plt.legend()\n",
    "plt.title('Train/validation loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model and optimizer\n",
    "#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "model.compile(optimizer=keras.optimizers.Adam(0.001),\n",
    "              loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "validation_steps = 20\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "# This callback will stop the training when there is no improvement in\n",
    "# the validation loss for three consecutive epochs.\n",
    "# train\n",
    "history=model.fit(train_dataset, epochs=epochs,\n",
    "                  #callbacks=[callback],\n",
    "          validation_data=val_dataset, verbose=1, validation_steps=validation_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "#scores = model.evaluate(test_dataset, verbose=1)\n",
    "#print(\"Final test loss and accuracy :\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# model.save('vggfish.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model/vggfish20191125AQU_MERGE.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model/vggfish20191125AQU_MERGE.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract feature\n",
    "## SESSION3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_data_dir ='/media/xingbo/Storage/fish_identification/data/SESSION_AQUARIUM/SESSION3'\n",
    "BATCH_SIZE = 8\n",
    "testloadData = LoadFishDataUtil(test_data_dir,BATCH_SIZE,IMG_WIDTH,IMG_HEIGHT,CLASS_NAMES)\n",
    "sess2_test_dataset,sess2_class_num = testloadData.loadTestFishData()\n",
    "test_num_elements = tf.data.experimental.cardinality(sess2_test_dataset).numpy()\n",
    "print(f\"we have total {test_num_elements} batches of images for testing, around {test_num_elements*BATCH_SIZE} samples\")\n",
    "# evaluate on test set\n",
    "#scores = model.evaluate(sess2_test_dataset, verbose=1)\n",
    "#print(\"Final test loss and accuracy :\", scores)\n",
    "from numpy import linalg as LA\n",
    "feats = []\n",
    "names = []\n",
    "feature_model = models.Model(inputs=model.input, outputs=model.get_layer('batch_normalization_13').output)\n",
    "n = 0\n",
    "for image_batch, label_batch in sess2_test_dataset:\n",
    "    feature=model(image_batch)\n",
    "    \n",
    "    #print(n)\n",
    "    #print(feature.shape[0])\n",
    "    for i in range(feature.shape[0]):\n",
    "        n=n+1\n",
    "        feats.append(feature[i])\n",
    "        names.append(np.argwhere(label_batch[i]).ravel())\n",
    "        indxmax=np.argmax(feature[i])\n",
    "        #print('predictions max index:',indxmax)\n",
    "        #print('predictions:', CLASS_NAMES[indxmax] )\n",
    "        #print('real:', CLASS_NAMES[np.argwhere(label_batch[i]).ravel()])\n",
    "       \n",
    "print(f\"finanly we have {n} samples extracted features\")\n",
    "feats3 = np.array(feats)\n",
    "names3 = np.array(names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dir ='/media/xingbo/Storage/fish_identification/data/SESSION_AQUARIUM/SESSION2'\n",
    "BATCH_SIZE = 8\n",
    "testloadData = LoadFishDataUtil(test_data_dir,BATCH_SIZE,IMG_WIDTH,IMG_HEIGHT,CLASS_NAMES)\n",
    "sess2_test_dataset,sess2_class_num = testloadData.loadTestFishData()\n",
    "test_num_elements = tf.data.experimental.cardinality(sess2_test_dataset).numpy()\n",
    "print(f\"we have total {test_num_elements} batches of images for testing, around {test_num_elements*BATCH_SIZE} samples\")\n",
    "\n",
    "feats = []\n",
    "names = []\n",
    "feature_model = models.Model(inputs=model.input, outputs=model.get_layer('batch_normalization_13').output)\n",
    "n = 0\n",
    "for image_batch, label_batch in sess2_test_dataset:\n",
    "    feature=model(image_batch)\n",
    "    for i in range(feature.shape[0]):\n",
    "        n=n+1\n",
    "        feats.append(feature[i])\n",
    "        names.append(np.argwhere(label_batch[i]).ravel())\n",
    "        indxmax=np.argmax(feature[i])\n",
    "       \n",
    "print(f\"finanly we have {n} samples extracted features\")\n",
    "feats2 = np.array(feats)\n",
    "names2 = np.array(names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dir ='/media/xingbo/Storage/fish_identification/data/SESSION_AQUARIUM/SESSION1_LT'\n",
    "BATCH_SIZE = 8\n",
    "testloadData = LoadFishDataUtil(test_data_dir,BATCH_SIZE,IMG_WIDTH,IMG_HEIGHT,CLASS_NAMES)\n",
    "sess2_test_dataset,sess2_class_num = testloadData.loadTestFishData()\n",
    "test_num_elements = tf.data.experimental.cardinality(sess2_test_dataset).numpy()\n",
    "print(f\"we have total {test_num_elements} batches of images for testing, around {test_num_elements*BATCH_SIZE} samples\")\n",
    "\n",
    "feats = []\n",
    "names = []\n",
    "feature_model = models.Model(inputs=model.input, outputs=model.get_layer('batch_normalization_13').output)\n",
    "n = 0\n",
    "for image_batch, label_batch in sess2_test_dataset:\n",
    "    feature=model(image_batch)\n",
    "    for i in range(feature.shape[0]):\n",
    "        n=n+1\n",
    "        feats.append(feature[i])\n",
    "        names.append(np.argwhere(label_batch[i]).ravel())\n",
    "        indxmax=np.argmax(feature[i])\n",
    "       \n",
    "print(f\"finanly we have {n} samples extracted features\")\n",
    "feats1 = np.array(feats)\n",
    "names1 = np.array(names)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asciiList = [n.encode(\"ascii\", \"ignore\") for n in names]\n",
    "# directory for storing extracted features\n",
    "feats_save_path = '/media/xingbo/Storage/fish_identification/data/merge3session.h5'\n",
    "print (\"--------------------------------------------------\")\n",
    "print (\"      writing feature extraction results ...\")\n",
    "print (\"--------------------------------------------------\")\n",
    "\n",
    "h5f = h5py.File(feats_save_path, 'w')\n",
    "h5f.create_dataset('Session3_features_merge3session', data=feats3)\n",
    "h5f.create_dataset('Session3_names_merge3session', data=names3)\n",
    "\n",
    "h5f.create_dataset('Session2_features_merge3session', data=feats2)\n",
    "h5f.create_dataset('Session2_names_merge3session', data=names2)\n",
    "\n",
    "h5f.create_dataset('Session1_features_merge3session', data=feats1)\n",
    "h5f.create_dataset('Session1_names_merge3session', data=names1)\n",
    "\n",
    "np.save('/media/xingbo/Storage/fish_identification/data/CLASS_NAMES.npy', CLASS_NAMES)\n",
    "\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feats_save_path = '/media/xingbo/Storage/fish_identification/data/merge3session.h5'\n",
    "print (\"--------------------------------------------------\")\n",
    "print (\"      reading feature extraction results ...\")\n",
    "print (\"--------------------------------------------------\")\n",
    "\n",
    "h5f = h5py.File(feats_save_path, 'r')\n",
    "Session1_features = h5f['Session1_features_merge3session'][:]\n",
    "Session1_names = h5f['Session1_names_merge3session'][:]\n",
    "\n",
    "Session2_features = h5f['Session2_features_merge3session'][:]\n",
    "Session2_names = h5f['Session2_names_merge3session'][:]\n",
    "\n",
    "Session3_features = h5f['Session3_features_merge3session'][:]\n",
    "Session3_names = h5f['Session3_names_merge3session'][:]\n",
    "\n",
    "h5f.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# matching session1 vs session 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract query image's feature, compute simlarity score and sort\n",
    "scores = np.dot(Session1_features, Session2_features.T)\n",
    "print(scores.shape)\n",
    "from numpy.linalg import norm\n",
    "res = 1 - np.dot(Session1_features/norm(Session1_features, axis=1)[...,None],(Session2_features/norm(Session2_features,axis=1)[...,None]).T)\n",
    "res = 1-res/2\n",
    "lable = Session1_names == Session2_names.T\n",
    "lable2 = Session1_names != Session2_names.T\n",
    "\n",
    "gscores=res[lable]\n",
    "print(gscores.shape)\n",
    "iscores=res[lable2]\n",
    "print(iscores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bob.measure\n",
    "iscores1vs2 = iscores.astype('float64')\n",
    "gscores1vs2=gscores.astype('float64') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EER = bob.measure.eer(iscores1vs2, gscores1vs2)\n",
    "print(f\"we can achieve EER with {EER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "# we assume you have your negatives and positives already split\n",
    "npoints = 100\n",
    "bob.measure.plot.roc(iscores1vs2, gscores1vs2, npoints, color=(0,0,0), linestyle='-', label='test') \n",
    "pyplot.xlabel('FPR (%)') \n",
    "pyplot.ylabel('FNR (%)') \n",
    "pyplot.grid(True)\n",
    "pyplot.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# matching session1 vs session 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract query image's feature, compute simlarity score and sort\n",
    "scores = np.dot(Session1_features, Session3_features.T)\n",
    "print(scores.shape)\n",
    "from numpy.linalg import norm\n",
    "res = 1 - np.dot(Session1_features/norm(Session1_features, axis=1)[...,None],(Session3_features/norm(Session3_features,axis=1)[...,None]).T)\n",
    "res = 1-res/2\n",
    "lable = Session1_names == Session3_names.T\n",
    "lable2 = Session1_names != Session3_names.T\n",
    "\n",
    "gscores=res[lable]\n",
    "print(gscores.shape)\n",
    "iscores=res[lable2]\n",
    "print(iscores.shape)\n",
    "\n",
    "import bob.measure\n",
    "iscores1vs3 = iscores.astype('float64')\n",
    "gscores1vs3=gscores.astype('float64') \n",
    "EER = bob.measure.eer(iscores1vs3, gscores1vs3)\n",
    "print(f\"we can achieve EER with {EER}\")\n",
    "from matplotlib import pyplot\n",
    "# we assume you have your negatives and positives already split\n",
    "npoints = 100\n",
    "bob.measure.plot.roc(iscores1vs3, gscores1vs3, npoints, color=(0,0,0), linestyle='-', label='test') \n",
    "pyplot.xlabel('FPR (%)') \n",
    "pyplot.ylabel('FNR (%)') \n",
    "pyplot.grid(True)\n",
    "pyplot.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# matching session2 vs session 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract query image's feature, compute simlarity score and sort\n",
    "scores = np.dot(Session2_features, Session3_features.T)\n",
    "print(scores.shape)\n",
    "from numpy.linalg import norm\n",
    "res = 1 - np.dot(Session2_features/norm(Session2_features, axis=1)[...,None],(Session3_features/norm(Session3_features,axis=1)[...,None]).T)\n",
    "res = 1-res/2\n",
    "lable = Session2_names == Session3_names.T\n",
    "lable2 = Session2_names != Session3_names.T\n",
    "\n",
    "gscores=res[lable]\n",
    "print(gscores.shape)\n",
    "iscores=res[lable2]\n",
    "print(iscores.shape)\n",
    "\n",
    "import bob.measure\n",
    "iscores2vs3 = iscores.astype('float64')\n",
    "gscores2vs3=gscores.astype('float64') \n",
    "EER = bob.measure.eer(iscores2vs3, gscores2vs3)\n",
    "print(f\"we can achieve EER with {EER}\")\n",
    "from matplotlib import pyplot\n",
    "# we assume you have your negatives and positives already split\n",
    "npoints = 100\n",
    "bob.measure.plot.roc(iscores2vs3, gscores2vs3, npoints, color=(0,0,0), linestyle='-', label='test') \n",
    "pyplot.xlabel('FPR (%)') \n",
    "pyplot.ylabel('FNR (%)') \n",
    "pyplot.grid(True)\n",
    "pyplot.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "# we assume you have your negatives and positives already split\n",
    "npoints = 100\n",
    "bob.measure.plot.roc(iscores1vs2, gscores1vs2, npoints, color=(0,0,0), linestyle='-', label='1 vs 2') \n",
    "bob.measure.plot.roc(iscores1vs3, gscores1vs3, npoints, color=(0,0,0), linestyle='-.', label='1 vs 3') \n",
    "bob.measure.plot.roc(iscores2vs3, gscores2vs3, npoints, color=(0,0,0), linestyle=':', label='2 vs 3') \n",
    "pyplot.xlabel('FPR (%)') \n",
    "pyplot.ylabel('FNR (%)') \n",
    "pyplot.legend()\n",
    "pyplot.grid(True)\n",
    "pyplot.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# intra session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bob.measure\n",
    "from matplotlib import pyplot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "\n",
    "res = 1 - np.dot(Session1_features/norm(Session1_features, axis=1)[...,None],(Session1_features/norm(Session1_features,axis=1)[...,None]).T)\n",
    "res = 1-res/2\n",
    "lable = Session1_names == Session1_names.T\n",
    "lable2 = Session1_names != Session1_names.T\n",
    "\n",
    "gscores=res[lable]\n",
    "iscores=res[lable2]\n",
    "\n",
    "iscores1vs1 = iscores.astype('float64')\n",
    "gscores1vs1=gscores.astype('float64') \n",
    "EER1vs1 = bob.measure.eer(iscores1vs1, gscores1vs1)\n",
    "print(f\"we can achieve EER with {EER1vs1}\")\n",
    "\n",
    "res = 1 - np.dot(Session2_features/norm(Session2_features, axis=1)[...,None],(Session2_features/norm(Session2_features,axis=1)[...,None]).T)\n",
    "res = 1-res/2\n",
    "lable = Session2_names == Session2_names.T\n",
    "lable2 = Session2_names != Session2_names.T\n",
    "\n",
    "gscores=res[lable]\n",
    "iscores=res[lable2]\n",
    "\n",
    "iscores2vs2 = iscores.astype('float64')\n",
    "gscores2vs2=gscores.astype('float64') \n",
    "EER2vs2 = bob.measure.eer(iscores2vs2, gscores2vs2)\n",
    "print(f\"we can achieve EER with {EER2vs2}\")\n",
    "\n",
    "res = 1 - np.dot(Session3_features/norm(Session3_features, axis=1)[...,None],(Session3_features/norm(Session3_features,axis=1)[...,None]).T)\n",
    "res = 1-res/2\n",
    "lable = Session3_names == Session3_names.T\n",
    "lable2 = Session3_names != Session3_names.T\n",
    "\n",
    "gscores=res[lable]\n",
    "iscores=res[lable2]\n",
    "\n",
    "iscores3vs3 = iscores.astype('float64')\n",
    "gscores3vs3=gscores.astype('float64') \n",
    "EER3vs3 = bob.measure.eer(iscores3vs3, gscores3vs3)\n",
    "print(f\"we can achieve EER with {EER3vs3}\")\n",
    "\n",
    "\n",
    "from matplotlib import pyplot\n",
    "# we assume you have your negatives and positives already split\n",
    "npoints = 100\n",
    "bob.measure.plot.roc(iscores1vs1, gscores1vs1, npoints, color=(0,0,0), linestyle='-', label='inta-session1') \n",
    "bob.measure.plot.roc(iscores2vs2, gscores2vs2, npoints, color=(0,0,0), linestyle='-.', label='inta-session2') \n",
    "bob.measure.plot.roc(iscores3vs3, gscores3vs3, npoints, color=(0,0,0), linestyle=':', label='inta-session3') \n",
    "pyplot.xlabel('FPR (%)') \n",
    "pyplot.ylabel('FNR (%)') \n",
    "pyplot.legend()\n",
    "pyplot.grid(True)\n",
    "pyplot.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
